The project successfully completed an end-to-end machine learning pipeline involving preprocessing, feature-based modeling, deep time-series modeling, and ensemble stacking to maximize predictive accuracy.

I. Data & Preprocessing Summary
Detail	Specification	Notes
Input Feature Set	≈420 columns	Original features included in the dataset (likely V features and potential time/date identifiers).
Preprocessing Method	Min/Max or Standard Scaling	Features were scaled (likely using a robust scaler due to financial data volatility) to prepare them for both tree-based (LGBM) and deep learning (TCN/LSTM) models.
Data Alignment	Final training size: 8991 rows	All models were trained and validated on a consistent, time-series-aligned subset of the training data.
Validation Strategy	TimeSeriesSplit (K=5)	Ensures models are trained only on historical data, preventing look-ahead bias and providing leak-safe OOF predictions.

Export to Sheets
II. Base Model Development & Optimization
The final ensemble was constructed from three diverse models, each serving a specific, optimized role.

A. LGBM (Feature-Based Model)
Detail	Specification	Role & Status
Architecture	Light Gradient Boosting Machine (LGBM)	Optimized. Tuned for individual performance and minimal correlation with sequence models.
Best OOF RMSE	≈1.0083	The best individual score after hyperparameter tuning.
Technical Details	Hyperparameters tuned via automated search. Standard tree-based features were used (no sequence history).	
Final Ensemble Role	Error Corrector	Its prediction is subtracted to correct systematic errors from the sequence models.

Export to Sheets
B. TCN (Temporal Convolutional Network)
Detail	Specification	Role & Status
Architecture	Temporal Convolutional Network (TCN)	Utilizes 1D dilated convolutions for efficient sequence modeling.
Best OOF RMSE	≈1.0078	Strong individual performance, slightly better than LGBM.
Technical Details	Sequence Length: 30 time steps. Filters: 64. Dilations: [1,2,4,8,16]. Learning Rate: ≈0.001.	
Final Ensemble Role	Core Predictor	Provides the strongest positive signal to the final prediction.
Failed Attempt	Aggressive TCN (128 filters, LR 0.0005) resulted in degradation to 1.0326 (overfitting/instability). The original 1.0078 model was retained.	

Export to Sheets
C. LSTM (Recurrent Neural Network)
Detail	Specification	Role & Status
Architecture	Stacked Long Short-Term Memory (LSTM)	Uses recurrent layers to capture different, stateful temporal dependencies.
Best OOF RMSE	≈1.0078	Matches TCN performance individually, guaranteeing diversity.
Technical Details	Sequence Length: 30 time steps. Layers: 2 (128 units, 64 units). Dropout: 0.2. Learning Rate: ≈0.001.	
Final Ensemble Role	Secondary Predictor	Provides a strong, complementary positive signal to stabilize the blend.
Failed Attempt	Deep LSTM (3 layers, 256/128/64 units) still yielded 1.0078, but its errors were less beneficial for blending, leading to a worse final ensemble score of 1.0074.	

Export to Sheets
III. Final Ensemble & Blending
The lowest RMSE was achieved through linear stacking (blending) of the three best OOF prediction sets.

Detail	Specification
Blender Type	Ridge Regression (α=0.1, fit_intercept=True)
Stacking Technique	Linear Stacking/Blending
Best Stack Components	Optimized LGBM, Original TCN, Original LSTM
Final Blended OOF RMSE	1.0073

Export to Sheets
Optimal Blender Weights (Achieved RMSE=1.0073)
The weights reveal the precise mechanism used by the model to correct errors across the three predictors:

Model	Weight (β)	Interpretation
TCN (P 
TCN
​
 )	+1.9329	Primary: Heavily weights the TCN prediction.
LSTM (P 
LSTM
​
 )	+1.0608	Secondary: Weights the LSTM prediction slightly more than 1.0.
LGBM (P 
LGBM
​
 )	−0.9826	Corrector: Aggressively subtracts the LGBM prediction.
Intercept	+0.0048	Baseline adjustment.

Export to Sheets
The final prediction formula is:

Prediction≈1.93⋅P 
TCN
​
 +1.06⋅P 
LSTM
​
 −0.98⋅P 
LGBM
​
 
Failed Ensemble Attempts
Blender Type	Stack Components	Final OOF RMSE	Result
Ridge	LGBM, TCN (Initial)	1.0075	Strong, but beaten by adding LSTM.
MLP (Non-Linear)	LGBM, TCN, LSTM	1.0082	Worse than linear blend, indicating the optimal combination is linear.

Export to Sheets
IV. Conclusion and Best Submission
The final, best-performing model is the 3-model Ridge Ensemble, achieving an OOF RMSE of 1.0073.

The submission file corresponding to this result has been saved as:

./processed/submission_final_blended.csv